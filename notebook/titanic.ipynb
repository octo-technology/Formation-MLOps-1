{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "381ac8a1-9f18-f1bd-67ed-f165373c8d0f"
   },
   "source": [
    "### The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76f20215-7870-47db-e2f0-c253a43aa8db"
   },
   "source": [
    "We are given information about a subset of the Titanic population and asked to build a predictive model that tells us whether or not a given passenger survived the shipwreck. We are given 10 basic explanatory variables, including passenger gender, age, and price of fare, among others. More details about the competition can be found on the Kaggle site, [here](https://www.kaggle.com/c/titanic). This is a classic binary classification problem, and we will be implementing a random forest classifer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This __notebook__ has been *uglified* for the purpose of our training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0941247d-c2f1-a753-e4ff-583a88f2e7dc"
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e99bef63-bd89-42e9-6897-aba6337b2afb"
   },
   "source": [
    "The goal of this section is to gain an understanding of our data in order to inform what we do in the feature engineering section.  \n",
    "\n",
    "We begin our exploratory data analysis by loading our standard modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e6e109b-469e-4210-18a9-59d56448fddc"
   },
   "source": [
    "We then load the data, which we have downloaded from the Kaggle website ([here](https://www.kaggle.com/c/titanic/data) is a link to the data if you need it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "e117a178-539a-d880-ab8c-5306d6d671f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os as Os\n",
    "\n",
    "df = pd.read_csv(Os.path.join('../input', 'train.csv'))\n",
    "df1 = pd.read_csv(Os.path.join('../input', 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d67e8e9f-d809-7138-9ebc-c8c9fa1fb88c"
   },
   "source": [
    "First, let's take a look at the summary of all the data. Immediately, we note that `Age`, `Cabin`, and `Embarked` have nulls that we'll have to deal with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "166004fb-0092-7fb9-f890-1b764a7f6da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "928cc3b0-b08d-cd49-0c0a-993f51cc5070"
   },
   "source": [
    "It appears that we can drop the `PassengerId` column, since it is merely an index. Note, however, that some people have reportedly improved their score with the `PassengerId` column. However, my cursory attempt to do so did not yield positive results, and moreover I would like to mimic a real-life scenario, where an index of a dataset generally has no correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "2fee872c-6233-57b1-d6a8-d017ef15edbd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "792b218a-b4c0-7a4a-2442-f375deee3581"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a69433b7-96b7-589b-4481-180206c1e5b2"
   },
   "source": [
    "Having done our cursory exploration of the variables, we now have a pretty good idea of how we want to transform our variables in preparation for our final dataset. We will perform our feature engineering through a series of helper functions that each serve a specific purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "906a9892-fe2a-ecd9-304d-ff4e63876867"
   },
   "source": [
    "This first function creates two separate columns: a numeric column indicating the length of a passenger's `Name` field, and a categorical column that extracts the passenger's title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "ada87e93-db17-8a39-0025-5aea95b8e684",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def names(train, test):\n",
    "    for i in [train, test]:\n",
    "        #i['Name_Max'] = i['Name'].apply(lambda x: max(x))\n",
    "        i['Name_Len'] = i['Name'].apply(lambda x: len(x))\n",
    "        i['Name_Title'] = i['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n",
    "        del i['Name']\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1a9f9d8b-eded-86e5-dd3f-a88e347b5135"
   },
   "source": [
    "Next, we impute the null values of the `Age` column by filling in the mean value of the passenger's corresponding title and class. This more granular approach to imputation should be more accurate than merely taking the mean age of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "8a265d21-ca70-1699-21fd-c098689026fc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def age_impute(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Age_Null_Flag'] = i['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "        data = train.groupby(['Name_Title', 'Pclass'])['Age']\n",
    "        i['Age'] = data.transform(lambda x: x.fillna(x.mean()))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c8285f4f-4f76-6188-c8df-bef76efb3800"
   },
   "source": [
    "We combine the `SibSp` and `Parch` columns into a new variable that indicates family size, and group the family size variable into three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "3f1b5572-f9e6-0bc5-4666-ba160eb4209e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in [df, df1]:\n",
    "    i['Fam_Size'] = np.where((i['SibSp']+i['Parch']) == 0 , 'Solo',\n",
    "                       np.where((i['SibSp']+i['Parch']) <= 3,'Nuclear', 'Big'))\n",
    "    del i['SibSp']\n",
    "    del i['Parch']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14acf849-b512-9eb4-37e3-427fc887b7c2"
   },
   "source": [
    "The `Ticket` column is used to create two new columns: `Ticket_Lett`, which indicates the first letter of each ticket (with the smaller-n values being grouped based on survival rate); and `Ticket_Len`, which indicates the length of the `Ticket` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "cbba884d-50f4-1e60-05bf-ed5dfb334fb6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ma_col = \"Ticket_Lett\"\n",
    "def ticket_grouped(train, test):\n",
    "    for i in [train, test]:\n",
    "        i[ma_col] = i['Ticket'].apply(lambda x: str(x)[0])\n",
    "        i[ma_col] = i['Ticket_Lett'].apply(lambda x: str(x))\n",
    "        i[ma_col] = np.where((i['Ticket_Lett']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), i['Ticket_Lett'],\n",
    "                                   np.where((i['Ticket_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']),\n",
    "                                            'Low_ticket', 'Other_ticket'))\n",
    "        i['Ticket_Len'] = i['Ticket'].apply(lambda x: len(x))\n",
    "        del i['Ticket']\n",
    "    return train, test\n",
    "def cabin(train,                             test):\n",
    "    for i in [train, test]:\n",
    "        i['Cabin_Letter'] = i['Cabin'].apply(lambda x: str(x)[0])\n",
    "        del i['Cabin']\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "# essai pour printer les clés d'un dictionnaire, à supprimer pour la prod\n",
    "my_dict = {'a':1, 'b':2}\n",
    "for k in my_dict.keys():\n",
    "    print(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a240a933-c6ed-63c4-7e00-1b9f4eba5c89"
   },
   "source": [
    "The following two functions extract the first letter of the `Cabin` column and its number, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "ebf50fb2-9cc7-3ace-a03a-8f7d419583a9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [df, df1]:\n",
    "    i['NuméroDeCbine1'] = i['Cabin'].apply(lambda x: str(x).split(' ')[-1][1:])\n",
    "    i['NuméroDeCbine1'].replace('an', np.NaN, inplace = True)\n",
    "    i['NuméroDeCbine1'] = i['NuméroDeCbine1'].apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)\n",
    "    i['NuméroDeCabin'] = pd.qcut(df['NuméroDeCbine1'],3)\n",
    "df = pd.concat((df, pd.get_dummies(df['NuméroDeCabin'], prefix = 'NuméroDeCabin')), axis = 1)\n",
    "df1 = pd.concat((df1, pd.get_dummies(df1['NuméroDeCabin'], prefix = 'NuméroDeCabin')), axis = 1)\n",
    "del df['NuméroDeCabin']\n",
    "del df1['NuméroDeCabin']\n",
    "del df['NuméroDeCbine1']\n",
    "del df1['NuméroDeCbine1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "194fdc08-b381-5d25-ab97-575bc257e465"
   },
   "source": [
    "We fill the null values in the `Embarked` column with the most commonly occuring value, which is 'S.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "025fe0a4-f682-d58b-b5d8-aa1577ffd2ab",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embarked_impute(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Embarked'] = i['Embarked'].fillna('S')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "215ae187-47cf-9233-c59c-25c643fef74e"
   },
   "source": [
    "We also fill in the one missing value of `Fare` in our test set with the mean value of `Fare` from the training set (transformations of test set data must always be fit using training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "2df1b2a7-f75c-ffa5-3bdd-636122ccb3f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1['Fare'].fillna(df['Fare'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "594708ed-a233-dbbe-4f70-2d1e342ddad2"
   },
   "source": [
    "Next, because we are using scikit-learn, we must convert our categorical columns into dummy variables. The following function does this, and then it drops the original categorical columns. It also makes sure that each category is present in both the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "a0cc99a9-1536-ee17-1249-48f4471a9e11",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Lett', 'Cabin_Letter', 'Name_Title', 'Fam_Size']):\n",
    "    for column in columns:\n",
    "        train[column] = train[column].apply(lambda x: str(x))\n",
    "        test[column] = test[column].apply(lambda x: str(x))\n",
    "        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]\n",
    "        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[good_cols]), axis = 1)\n",
    "        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[good_cols]), axis = 1)\n",
    "        del train[column]\n",
    "        del test[column]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c8b9d7eb-3afc-4082-f7b8-80d043c4f830"
   },
   "source": [
    "Our last helper function drops any columns that haven't already been dropped. In our case, we only need to drop the `PassengerId` column, which we have decided is not useful for our problem (by the way, I've confirmed this with a separate test). Note that dropping the `PassengerId` column here means that we'll have to load it later when creating our submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "9680ae7e-5ae4-7c45-3ae1-0781b5f383eb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop(train, test, bye = ['PassengerId']):\n",
    "    for i in [train, test]:\n",
    "        for z in bye:\n",
    "            del i[z]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename vars\n",
    "train = df\n",
    "test = df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8d1efbbb-e0dd-f3d4-acd3-ef9eb5c396e2"
   },
   "source": [
    "Having built our helper functions, we can now execute them in order to build our dataset that will be used in the model:a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "d004f91d-1c6b-e281-6e4c-45b44eadbcca"
   },
   "outputs": [],
   "source": [
    "train, test = names(train, test)\n",
    "train, test = age_impute(train, test)\n",
    "train, test = cabin(train, test)\n",
    "train, test = embarked_impute(train, test)\n",
    "test['Fare'].fillna(train['Fare'].mean(), inplace = True)\n",
    "train, test = ticket_grouped(train, test)\n",
    "train, test = dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Lett',\n",
    "                                                                     'Cabin_Letter', 'Name_Title', 'Fam_Size'])\n",
    "train, test = drop(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35d843bc-3607-f11a-55f9-2828cf5eb91e"
   },
   "source": [
    "We can see that our final dataset has 45 columns, composed of our target column and 44 predictor variables. Although highly dimensional datasets can result in high variance, I think we should be fine here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "09391302-b621-4730-7589-7eb017286e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1066e65e-e578-e896-5c38-1457a947ec6f"
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "32b4e910-cbe5-04c6-4383-c6f02483e595"
   },
   "source": [
    "We will use grid search to identify the optimal parameters of our random forest model. Because our training dataset is quite small, we can get away with testing a wider range of hyperparameter values. When I ran this on my 8 GB Windows machine, the process took less than ten minutes. I will not run it here for the sake of saving myself time, but I will discuss the results of this grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f6c54fa-033e-075f-0e86-c9c0b469a03b"
   },
   "source": [
    "from sklearn.model_selection import GridSearchCV  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_features='auto',\n",
    "                                oob_score=True,\n",
    "                                random_state=1,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "param_grid = { \"criterion\"   : [\"gini\", \"entropy\"],\n",
    "             \"min_samples_leaf\" : [1, 5, 10],\n",
    "             \"min_samples_split\" : [2, 4, 10, 12, 16],\n",
    "             \"n_estimators\": [50, 100, 400, 700, 1000]}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=3,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(train.iloc[:, 1:], train.iloc[:, 0])\n",
    "\n",
    "print(gs.best_score_)   \n",
    "print(gs.best_params_)  \n",
    "print(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "11038f38-44d4-0cbd-328b-1ad7196819fe"
   },
   "source": [
    "Looking at the results of the grid search:  \n",
    "\n",
    "0.838383838384  \n",
    "{'min_samples_split': 10, 'n_estimators': 700, 'criterion': 'gini', 'min_samples_leaf': 1}  \n",
    "\n",
    "...we can see that our optimal parameter settings are not at the endpoints of our provided values, meaning that we do not have to test more values. What else can we say about our optimal values? The `min_samples_split` parameter is at 10, which should help mitigate overfitting to a certain degree. This is especially good because we have a relatively large number of estimators (700), which could potentially increase our generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c95a0ff7-0f68-7e28-0c9c-36a52808f578"
   },
   "source": [
    "### Model Estimation and Evaluation<a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e494ad2b-92e3-782f-13c1-f53a86602298"
   },
   "source": [
    "We are now ready to fit our model using the optimal hyperparameters. The out-of-bag score can give us an unbiased estimate of the model accuracy, and we can see that the score is 82.94%, which is only a little higher than our final leaderboard score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "5593980a-4145-9594-299c-f4d1a9f01970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(criterion='gini', n_estimators=700, min_samples_split=10, min_samples_leaf=1, max_features='auto',oob_score=True,random_state=1, n_jobs=-1)\n",
    "rf.fit(train.iloc[:, 1:], train.iloc[:, 0])\n",
    "print(\"%.4f\" % rf.oob_score_)\n",
    "\n",
    "# Predict\n",
    "mesPredictions = rf.predict(test)\n",
    "mesPredictions = pd.DataFrame(mesPredictions, columns=['Survived'])\n",
    "test = pd.read_csv(os.path.join('../input', 'test.csv'))\n",
    "mesPredictions = pd.concat((test.iloc[:, 0], mesPredictions), axis = 1)\n",
    "mesPredictions.to_csv('y_test15.csv', sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'selection'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-805920f7ce26>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Var selection\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mlist_of_var\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mavancee\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: module 'sklearn' has no attribute 'selection'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Var selection\n",
    "import sklearn\n",
    "list_of_var = sklearn.selection.avancee(train, test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8361\n"
     ]
    }
   ],
   "source": [
    "# essai avec 600 arbres au lieu de 700\n",
    "rf = RandomForestClassifier(criterion='gini', \n",
    "                             n_estimators=600,\n",
    "                             min_samples_split=10,\n",
    "                             min_samples_leaf=1,\n",
    "                             max_features='auto',\n",
    "                             oob_score=True,\n",
    "                             random_state=1,\n",
    "                             n_jobs=-1)\n",
    "rf.fit(train.iloc[:, 1:], train.iloc[:, 0])\n",
    "print(\"%.4f\" % rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4b44766d-6974-b7f3-b801-eb5d9423ae49"
   },
   "source": [
    "Let's take a brief look at our variable importance according to our random forest model. We can see that some of the original columns we predicted would be important in fact were, including gender, fare, and age. But we also see title, name length, and ticket length feature prominently, so we can pat ourselves on the back for creating such useful variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "d77e221b-352d-8669-05d9-f7defce05709"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sex_female</td>\n",
       "      <td>0.118741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>0.108694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Name_Title_Mr.</td>\n",
       "      <td>0.107232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fare</td>\n",
       "      <td>0.087679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Name_Len</td>\n",
       "      <td>0.081811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.080652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pclass_3</td>\n",
       "      <td>0.043106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Name_Title_Miss.</td>\n",
       "      <td>0.032693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ticket_Len</td>\n",
       "      <td>0.031916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cabin_Letter_n</td>\n",
       "      <td>0.028150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Name_Title_Mrs.</td>\n",
       "      <td>0.028002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Fam_Size_Big</td>\n",
       "      <td>0.026847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pclass_1</td>\n",
       "      <td>0.022322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Fam_Size_Nuclear</td>\n",
       "      <td>0.020798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ticket_Lett_1</td>\n",
       "      <td>0.018053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ticket_Lett_3</td>\n",
       "      <td>0.012243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Name_Title_Master.</td>\n",
       "      <td>0.012007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pclass_2</td>\n",
       "      <td>0.011725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Embarked_S</td>\n",
       "      <td>0.011455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ticket_Lett_Low_ticket</td>\n",
       "      <td>0.011227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  variable  importance\n",
       "12              Sex_female    0.118741\n",
       "11                Sex_male    0.108694\n",
       "33          Name_Title_Mr.    0.107232\n",
       "1                     Fare    0.087679\n",
       "5                 Name_Len    0.081811\n",
       "0                      Age    0.080652\n",
       "8                 Pclass_3    0.043106\n",
       "35        Name_Title_Miss.    0.032693\n",
       "7               Ticket_Len    0.031916\n",
       "25          Cabin_Letter_n    0.028150\n",
       "34         Name_Title_Mrs.    0.028002\n",
       "43            Fam_Size_Big    0.026847\n",
       "9                 Pclass_1    0.022322\n",
       "41        Fam_Size_Nuclear    0.020798\n",
       "19           Ticket_Lett_1    0.018053\n",
       "20           Ticket_Lett_3    0.012243\n",
       "36      Name_Title_Master.    0.012007\n",
       "10                Pclass_2    0.011725\n",
       "13              Embarked_S    0.011455\n",
       "23  Ticket_Lett_Low_ticket    0.011227"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat((pd.DataFrame(train.iloc[:, 1:].columns, columns = ['variable']), \n",
    "           pd.DataFrame(rf.feature_importances_, columns = ['importance'])), \n",
    "          axis = 1).sort_values(by='importance', ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4fbf72d-a7b6-1d14-73cb-7f763d291272"
   },
   "source": [
    "Our last step is to predict the target variable for our test data and generate an output file that will be submitted to Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14dc0e66-9fc4-86bf-8927-46d366d4bbcf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae3dcb9e-8e70-956a-a0eb-a5aa1f188a99"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This exercise is a good example of how far basic feature engineering can take you. It is worth mentioning that I did try various other models before arriving at this one. Some of the other variations I tried were different groupings for the categorical variables (plenty more combinations remain), linear discriminant analysis on a couple numeric columns, and eliminating more variables, among other things. This is a competition with a generous allotment of submission attempts, and as a result, it's quite possible that even the leaderboard score is an overestimation of the true quality of the model, since the leaderboard can act as more of a validation score instead of a true test score. \n",
    "\n",
    "I welcome any comments and suggestions."
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0.0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
